---
layout: post
title: go-bt-6
excerpt: "Write a BitTorrent client from scratch in Go: managing peers and pieces"
categories: [coding]
tags: [howto,golang]
---

We're now in a state whereby we can [handle](({% post_url 2024-11-16-go-bt-5 %})), that is request and receive, pieces. What we now need is a mechanism through which we can decide what piece to download next and a place to store it - the end goal being to download all pieces and make the original file(s) available.

We're building up from the [go-bt](https://github.com/axiomiety/go-bt) repository. Clone it if you want to follow along!

* TOC
{:toc}

# Piece selection

To fully parallelise downloads we want to request a different piece from each peer. And as soon as a piece is retrieved, we want to request the next one. But it's not necessarily (but it can be!) as simple as requesting a missing piece from a peer who has it. Take the following example:

```
piece: 0 1 2 3 4 5 6
peerA: X . X X X . X
peerB: . X . X X X X
```

Which pieces do you request first? Which peer do you request piece 3 from?

There are a few algorithms available - for instance we should probably prioritise piece 0, 1, 2 and 5 - as their availability is limited and if either peer goes offline, we won't be able to complete our download. But once those have been downloaded, what about pieces 3, 4, 6? Piece 6 is likely the smallest (it doesn't necessarily fill up the whole `piece length`), so we can probably keep it for last. Also some types of media files can be processed sequentially, even if it's missing later data.

For now we'll keep it simple. Iterating through all the pieces we need, we'll simply punt it to the first *available* peer (i.e. a peer needs to both have the piece as well as be in a good state).

{% highlight golang %}
{% endhighlight %}

# Piece storage

Once a piece has been downloaded we need to store it somewhere (keeping it in memory ain't it). Given how we're restricting ourselves to the standard library we don't exactly have tons of options. With this in mind the approach we'll take is to create empty files of the required sizes - and we'll write each piece at the corresponding offset.

It sounds easy enough in practice but pieces at file boundaries need to be handled carefully. Consider the following (contrived) example:

```
piece: |    10    |    10    |    10    |
files: |     12     |  4 |   7    |
```

Total size across all 3 files is 12 + 4 + 7 = 23, and for a piece size of 10 that will be split across 3 pieces.

Let's first write a test that replicates the above:
{% highlight golang %}
func TestGetSegmentsForPiece(t *testing.T) {
	// total size is 23 bytes for a total of 3 pieces
	binfo := &data.BEInfo{
		Files: []data.BEFile{
			{
				Path:   []string{"file1"},
				Length: 12,
			},
			{
				Path:   []string{"file2"},
				Length: 4,
			},
			{
				Path:   []string{"file3"},
				Length: 7,
			},
		},
		PieceLength: 10,
	}

	expected := map[int][]torrent.Segment{
		0: {
			{
				Filename: "file1",
				Offset:   uint64(0),
				Length:   10,
			}},
		// this is the most interesting piece - it spans 3 files!
		1: {
			{
				Filename: "file1",
				Offset:   uint64(10),
				Length:   2,
			},
			{
				Filename: "file2",
				Offset:   uint64(0),
				Length:   4,
			},
			{
				Filename: "file3",
				Offset:   uint64(0),
				Length:   4,
			}},
		2: {
			{
				Filename: "file3",
				Offset:   uint64(4),
				Length:   3,
			}},
	}

	for pieceIdx, expectedSegments := range expected {
		segments := torrent.GetSegmentsForPiece(binfo, uint64(pieceIdx))
		if !reflect.DeepEqual(segments, expectedSegments) {
			t.Errorf("expected %+v, got %+v ", expectedSegments, segments)
		}
	}
}
{% endhighlight %}

And this is the implementation of `GetSegmentsForPiece`. It's a linear search through the files - we could keep an intermediate datastructure to keep track of offsets between files but it's simple enough to recompute on the fly:

{% highlight golang %}
func GetSegmentsForPiece(i *data.BEInfo, index uint64) []Segment {
	segments := make([]Segment, 0)

	pieceStart := index * i.PieceLength
	bytesRemainingInPiece := i.PieceLength
	runningOffset := uint64(0)
	for _, file := range i.Files {
		if bytesRemainingInPiece == 0 || runningOffset > pieceStart+bytesRemainingInPiece {
			// we're done
			break
		} else if (runningOffset + uint64(file.Length)) < pieceStart {
			// this is beyond the current file's boundary
			runningOffset += uint64(file.Length)
		} else {
			// part of this piece belongs to this file
			fileBytesInPiece := min(runningOffset+uint64(file.Length)-pieceStart, bytesRemainingInPiece)
			segments = append(segments, Segment{
				Filename: file.Path[0],
				Offset:   pieceStart - runningOffset,
				Length:   fileBytesInPiece,
			})
			// this may well be 0 now
			bytesRemainingInPiece -= fileBytesInPiece
			pieceStart += fileBytesInPiece
			// if we're at a file boundary we should move on to the next one
			if pieceStart == (runningOffset + uint64(file.Length)) {
				runningOffset += uint64(file.Length)
			}
		}
	}
	return segments
}
{% endhighlight %}

Now that we can map a piece to different file segments, writing the contents out is relatively trivial:

{% highlight golang %}
func WriteSegments(segments []Segment, data []byte, baseDir string) {
	dataOffset := 0
	for _, segment := range segments {
		filePath := path.Join(baseDir, segment.Filename)
		file, err := os.OpenFile(filePath, os.O_RDWR|os.O_CREATE, 0666)
		common.Check(err)
		defer file.Close()
		writer := io.NewOffsetWriter(file, int64(segment.Offset))
		writer.Write(data[dataOffset : dataOffset+int(segment.Length)])
		dataOffset += int(segment.Length)
	}
}
{% endhighlight %}

# Managing peers

The `PeerHandler` handles all connectivity to a given peer, from the handshake down to message-passing. We can ask it to request a given piece and it will dutifully handle the back-and-forth with the peer. It also takes care of maintaining an internal state of which pieces the peer has available (further to the `bitfield` message, a peer can disseminate a `have` message when it comes in possession of a piece it didn't have before).

In an ideal world each torrent is served by multiple peers such that in aggregate, each piece in the torrent is available from at least one peer. The role of the "manager" is then to assign pieces to the relevant peers and collate those accordingly.

## The manager is... managing

Generally speaking the manager's role is to register with a tracker/get a list of peers, and manage a pool of peers which will change over time (some peers will be ejected, some peers will be added). Realistically we aren't going to connect to every single peer - and trackers usually only return a subset anyway - we'll establish a pool of say N peers. Out of those we'll need to filter for:
  - peers that contain pieces we're interested in
  - peers that unchoke us

That will be the subset of peers we can request pieces from. As time goes by we may find that a peer chokes us, or a peer no longer has pieces we're interested in. When that's the case we can eject the aforementioned peer and the manager will replace it with another.

This process will continue until we have managed to download every piece.

## What happens after?

Once all pieces have been downloaded/written, our torrent is now complete. We'll keep heartbeating to the tracker and other peers may connect to us to download pieces.

To be clear, this process can happen throughout the lifecycle of the torrent - the moment we have a piece available we should let

# Taking it further
