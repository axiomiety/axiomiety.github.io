---
layout: post
title: go-bt-6
excerpt: "Write a BitTorrent client from scratch in Go: managing peers and pieces"
categories: [coding]
tags: [howto,golang]
---

We're now in a state whereby we can [handle](({% post_url 2024-11-16-go-bt-5 %})), that is request and receive, pieces. What we now need is a mechanism through which we can decide what piece to download next and a place to store it - the end goal being to download all pieces and make the original file(s) available.

We're building up from the [go-bt](https://github.com/axiomiety/go-bt) repository. Clone it if you want to follow along!

* TOC
{:toc}

# Piece selection

To fully parallelise downloads we want to request a different piece from each peer. And as soon as a piece is retrieved, we want to request the next one. But it's not necessarily (but it can be!) as simple as requesting a missing piece from a peer who has it. Take the following example:

```
piece: 0 1 2 3 4 5 6
peerA: X . X X X . X
peerB: . X . X X X X
```

Which pieces do you request first? Which peer do you request piece 3 from?

There are a few algorithms available - for instance we should probably prioritise piece 0, 1, 2 and 5 - as their availability is limited and if either peer goes offline, we won't be able to complete our download. But once those have been downloaded, what about pieces 3, 4, 6? Piece 6 is likely the smallest (it doesn't necessarily fill up the whole `piece length`), so we can probably keep it for last. Also some types of media files can be processed sequentially, even if it's missing later data.

For now we'll keep it simple. Iterating through all the pieces we need, we'll simply punt it to the first *available* peer (i.e. a peer needs to both have the piece as well as be in a good state).

{% highlight golang %}
func (p *PeerManager) DownloadNextPiece() boolean {
	didAnything := false
	for pieceNum := range p.BitField.NumPieces() {
		if !p.BitField.HasPiece(pieceNum) {
			for peerId, handler := range p.PeerHandlers {
				if handler.State == UNCHOKED && handler.BitField.HasPiece(pieceNum) {
					log.Printf("peer %x is UNCHOKED and has piece %d", peerId, pieceNum)
					// usually we'd request PIECE_LENGTH, but if this is e.g. the last
					// piece, the size of the piece may be less than the piece size
					// specified in the info dict
					handler.RequestPiece(pieceNum, min(p.Torrent.Info.GetPieceSize(pieceNum), PIECE_LENGTH))
					didAnything := true
				}
			}
		}
	}
	return didAnything
}
{% endhighlight %}

# Piece storage

Once a piece has been downloaded we need to store it somewhere (keeping it in memory ain't it). Given how we're restricting ourselves to the standard library we don't exactly have tons of options. With this in mind the approach we'll take is to create empty files of the required sizes - and we'll write each piece at the corresponding offset.

It sounds easy enough in practice but pieces at file boundaries need to be handled carefully. Consider the following (contrived) example:

```
piece: |    10    |    10    |    10    |
files: |     12     |  4 |   7    |
```

Total size across all 3 files is 12 + 4 + 7 = 23, and for a piece size of 10 that will be split across 3 pieces.

Let's first write a test that replicates the above:
{% highlight golang %}
func TestGetSegmentsForPiece(t *testing.T) {
	// total size is 23 bytes for a total of 3 pieces
	binfo := &data.BEInfo{
		Files: []data.BEFile{
			{
				Path:   []string{"file1"},
				Length: 12,
			},
			{
				Path:   []string{"file2"},
				Length: 4,
			},
			{
				Path:   []string{"file3"},
				Length: 7,
			},
		},
		PieceLength: 10,
	}

	expected := map[int][]torrent.Segment{
		0: {
			{
				Filename: "file1",
				Offset:   uint64(0),
				Length:   10,
			}},
		// this is the most interesting piece - it spans 3 files!
		1: {
			{
				Filename: "file1",
				Offset:   uint64(10),
				Length:   2,
			},
			{
				Filename: "file2",
				Offset:   uint64(0),
				Length:   4,
			},
			{
				Filename: "file3",
				Offset:   uint64(0),
				Length:   4,
			}},
		2: {
			{
				Filename: "file3",
				Offset:   uint64(4),
				Length:   3,
			}},
	}

	for pieceIdx, expectedSegments := range expected {
		segments := torrent.GetSegmentsForPiece(binfo, uint32(pieceIdx))
		if !reflect.DeepEqual(segments, expectedSegments) {
			t.Errorf("expected %+v, got %+v ", expectedSegments, segments)
		}
	}
}
{% endhighlight %}

And this is the implementation of `GetSegmentsForPiece`. It's a linear search through the files - we could keep an intermediate datastructure to keep track of offsets between files but it's simple enough to recompute on the fly:

{% highlight golang %}
func GetSegmentsForPiece(i *data.BEInfo, index uint64) []Segment {
	segments := make([]Segment, 0)

	pieceStart := index * i.PieceLength
	bytesRemainingInPiece := i.PieceLength
	runningOffset := uint64(0)
	for _, file := range i.Files {
		if bytesRemainingInPiece == 0 || runningOffset > pieceStart+bytesRemainingInPiece {
			// we're done
			break
		} else if (runningOffset + uint64(file.Length)) < pieceStart {
			// this is beyond the current file's boundary
			runningOffset += uint64(file.Length)
		} else {
			// part of this piece belongs to this file
			fileBytesInPiece := min(runningOffset+uint64(file.Length)-pieceStart, bytesRemainingInPiece)
			segments = append(segments, Segment{
				Filename: file.Path[0],
				Offset:   pieceStart - runningOffset,
				Length:   fileBytesInPiece,
			})
			// this may well be 0 now
			bytesRemainingInPiece -= fileBytesInPiece
			pieceStart += fileBytesInPiece
			// if we're at a file boundary we should move on to the next one
			if pieceStart == (runningOffset + uint64(file.Length)) {
				runningOffset += uint64(file.Length)
			}
		}
	}
	return segments
}
{% endhighlight %}

Now that we can map a piece to different file segments, writing the contents out is relatively trivial:

{% highlight golang %}
func WriteSegments(segments []Segment, data []byte, baseDir string) {
	dataOffset := 0
	for _, segment := range segments {
		filePath := path.Join(baseDir, segment.Filename)
		file, err := os.OpenFile(filePath, os.O_RDWR|os.O_CREATE, 0666)
		common.Check(err)
		defer file.Close()
		writer := io.NewOffsetWriter(file, int64(segment.Offset))
		writer.Write(data[dataOffset : dataOffset+int(segment.Length)])
		dataOffset += int(segment.Length)
	}
}
{% endhighlight %}

# Torrent management

We have a number of pieces (haha) in place to build up to something that can download the full torrent.

## Managing a peer

The `PeerHandler` handles all connectivity to a given peer, from the handshake down to message-passing. We can ask it to request a given piece and it will dutifully handle the back-and-forth with the peer. It also takes care of maintaining an internal state of which pieces the peer has available (further to the `bitfield` message, a peer can disseminate a `have` message when it comes in possession of a piece it didn't have before).

Once a piece has been retrieved from a peer, it sets its state to `PIECE_COMPLETE`. The manager will periodically check each peer for this status - and write out the piece before setting the state back to `READY` (at which point it may be assigned another piece to download).

## Orchestrating

The end goal is to download every piece. The manager knows which pieces are required and can iterate through its pool of peers to find the ones that have them available. It is then a matter of dispatching a request to the relevant peer. Once a piece becomes available (the `PeerHandler` will be in a `PIECE_COMPLETE` state) we can write the piece to file, update our bitfield, and reset the state accordingly.

It is therefore important for us to manage the pool of peers effectively. For instance if all of the peers in our pool are chocked, or none carry the pieces we require, we should eject them from the pool. We periodically refresh the list of peers via the tracker (which is usually a subset of all available peers) - and can use newly found ones to replenish it.

Let's list down the steps. The order isn't terribly important as long as each task gets done.

  1. Get a list of peers from the tracker
  1. Refresh our pool of peers
  1. Request pieces from peers
  1. Write downloaded pieces

Some steps are a little more involved - e.g. refreshing our pool of peers can be as simple as ejecting chocked peers or having some sort of priority-based system:
  1. Ejecting choked peers that don't have any pieces we're interested in
  1. Ejecting peers that don't have any pieces we're interested in
  1. Ejecting choked peers that have the least number of pieces we're interested in
    1. But keeping choked peers that have rare pieces 

This allows us to rank peers, which makes it easier to decide which to eject. Let's say we have the following availability:
|pieceIdx|numPeersWithPiece|
|-|-|
|1|3|
|2|1|
|3|5|

Let's say we have a peer with piece 1 and 3, vs one with just 3. Clearly the former is deemed more important. But what about 1 and 3 vs 2? As we only have one peer with piece 2 and pieces 1 and 3 can be obtained from multiple peers it is deemed the most imporant.


## What happens after?

Once all pieces have been downloaded/written, our torrent is now complete. We'll keep heartbeating to the tracker and other peers may connect to us to download pieces.

To be clear, this process can happen throughout the lifecycle of the torrent - the moment we have a piece available we should let

# Taking it further

## `PeerHandler` state transitions

Currently the manager round-robins across peers and performs actions based on their state. The polling approach is relatively simple and works well when we need to find a peer for a specific piece, but we could definitely have a channel back to the manager to handle this more effectively (though if that channel is shared, the message would need to include the handler's ID or something similar to get).